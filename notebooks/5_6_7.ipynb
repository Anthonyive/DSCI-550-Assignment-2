{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "serial-rolling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# great example here: \n",
    "# https://idiotdeveloper.com/dcgan-implementing-deep-convolutional-generative-adversarial-network-in-tensorflow/\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smaller-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "IMG_H = 64\n",
    "IMG_W = 64\n",
    "IMG_C = 3\n",
    "batch_size = 128\n",
    "latent_dim = 128\n",
    "num_epochs = 60\n",
    "# images_path = glob(\"data/*\")\n",
    "images_path = glob('../../input/reconnaissance/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equivalent-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naval-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img - 127.5) / 127.5\n",
    "    # resize image\n",
    "    img = img[:, :, :3]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "white-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(images_path, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images_path)\n",
    "    dataset = dataset.shuffle(buffer_size=10240)\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "binding-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_block(inputs, num_filters, kernel_size, strides, bn=True):\n",
    "    x = Conv2DTranspose(\n",
    "        filters=num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        kernel_initializer=w_init,\n",
    "        padding=\"same\",\n",
    "        strides=strides,\n",
    "        use_bias=False\n",
    "        )(inputs)\n",
    "\n",
    "    if bn:\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "superior-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, num_filters, kernel_size, padding=\"same\", strides=2, activation=True):\n",
    "    x = Conv2D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        kernel_initializer=w_init,\n",
    "        padding=padding,\n",
    "        strides=strides,\n",
    "    )(inputs)\n",
    "\n",
    "    if activation:\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developing-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "    f = [2**i for i in range(5)][::-1]\n",
    "    filters = 32\n",
    "    output_strides = 16\n",
    "    h_output = IMG_H // output_strides\n",
    "    w_output = IMG_W // output_strides\n",
    "\n",
    "    noise = Input(shape=(latent_dim,), name=\"generator_noise_input\")\n",
    "\n",
    "    x = Dense(f[0] * filters * h_output * w_output, use_bias=False)(noise)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Reshape((h_output, w_output, 16 * filters))(x)\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        x = deconv_block(x,\n",
    "            num_filters=f[i] * filters,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            bn=True\n",
    "        )\n",
    "\n",
    "    x = conv_block(x,\n",
    "        num_filters=3,\n",
    "        kernel_size=5,\n",
    "        strides=1,\n",
    "        activation=False\n",
    "    )\n",
    "    fake_output = Activation(\"tanh\")(x)\n",
    "\n",
    "    return Model(noise, fake_output, name=\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medical-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    f = [2**i for i in range(4)]\n",
    "    image_input = Input(shape=(IMG_H, IMG_W, IMG_C))\n",
    "    x = image_input\n",
    "    filters = 64\n",
    "    output_strides = 16\n",
    "    h_output = IMG_H // output_strides\n",
    "    w_output = IMG_W // output_strides\n",
    "\n",
    "    for i in range(0, 4):\n",
    "        x = conv_block(x, num_filters=f[i] * filters, kernel_size=5, strides=2)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "\n",
    "    return Model(image_input, x, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alone-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        for _ in range(2):\n",
    "            ## Train the discriminator\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            generated_images = self.generator(random_latent_vectors)\n",
    "            generated_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "            with tf.GradientTape() as ftape:\n",
    "                predictions = self.discriminator(generated_images)\n",
    "                d1_loss = self.loss_fn(generated_labels, predictions)\n",
    "            grads = ftape.gradient(d1_loss, self.discriminator.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "            ## Train the discriminator\n",
    "            labels = tf.ones((batch_size, 1))\n",
    "\n",
    "            with tf.GradientTape() as rtape:\n",
    "                predictions = self.discriminator(real_images)\n",
    "                d2_loss = self.loss_fn(labels, predictions)\n",
    "            grads = rtape.gradient(d2_loss, self.discriminator.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        ## Train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        with tf.GradientTape() as gtape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = gtape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        return {\"d1_loss\": d1_loss, \"d2_loss\": d2_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "domestic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n):\n",
    "    examples = (examples + 1) / 2.0\n",
    "    for i in range(n * n):\n",
    "        pyplot.subplot(n, n, i+1)\n",
    "        pyplot.axis(\"off\")\n",
    "        pyplot.imshow(examples[i])\n",
    "    filename = f\"../models/samples/generated_plot_epoch-{epoch+1}.png\"\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "gothic-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_image(model, noise_dim=latent_dim):\n",
    "    test_input = tf.random.normal([1, noise_dim])\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.imshow((predictions[0, :, :, :] * 127.5 + 127.5) / 255.)\n",
    "    plt.axis('off') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "enormous-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_show_images(generator, noise_dim=latent_dim, rows=4, cols=4):\n",
    "    predictions = generator(tf.random.normal([16, noise_dim]))\n",
    "    print(predictions.shape)\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow((predictions[i, :, :, :] * 127.5 + 127.5) / 255.)\n",
    "        plt.axis('off') \n",
    "        \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "industrial-hughes",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5929e96503df4702b4943b54aa5422f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.6727 - d2_loss: 0.2659 - g_loss: 0.7327\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.3028 - d2_loss: 0.2485 - g_loss: 2.3247\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2215 - d2_loss: 0.2186 - g_loss: 3.0263\n",
      "4/4 [==============================] - 18s 5s/step - d1_loss: 0.2168 - d2_loss: 0.2157 - g_loss: 2.8972\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2265 - d2_loss: 0.2156 - g_loss: 2.7212\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2049 - d2_loss: 0.2358 - g_loss: 2.7665\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2412 - d2_loss: 0.2549 - g_loss: 3.0433\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2977 - d2_loss: 0.4537 - g_loss: 2.7422\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2138 - d2_loss: 0.2728 - g_loss: 2.4282\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2177 - d2_loss: 0.3029 - g_loss: 1.6786\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2068 - d2_loss: 0.2973 - g_loss: 2.4228\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2006 - d2_loss: 0.3479 - g_loss: 1.6349\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2171 - d2_loss: 0.3507 - g_loss: 2.2614\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2165 - d2_loss: 0.3057 - g_loss: 2.3946\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2130 - d2_loss: 0.3018 - g_loss: 2.4540\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2136 - d2_loss: 0.3312 - g_loss: 2.4588\n",
      "4/4 [==============================] - 19s 5s/step - d1_loss: 0.2089 - d2_loss: 0.3033 - g_loss: 2.2958\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2061 - d2_loss: 0.2951 - g_loss: 2.1201\n",
      "4/4 [==============================] - 18s 5s/step - d1_loss: 0.2084 - d2_loss: 0.2919 - g_loss: 2.4127\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2122 - d2_loss: 0.2957 - g_loss: 2.5011\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.3554 - d2_loss: 0.5548 - g_loss: 3.0673\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2184 - d2_loss: 0.3615 - g_loss: 2.1733\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2130 - d2_loss: 0.3207 - g_loss: 2.2617\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2101 - d2_loss: 0.3046 - g_loss: 2.3378\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2265 - d2_loss: 0.3374 - g_loss: 2.6106\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2109 - d2_loss: 0.3099 - g_loss: 2.2577\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2118 - d2_loss: 0.3053 - g_loss: 2.2841\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2052 - d2_loss: 0.2785 - g_loss: 2.3464\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2167 - d2_loss: 0.3333 - g_loss: 2.5245\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2078 - d2_loss: 0.2769 - g_loss: 2.3716\n",
      "4/4 [==============================] - 17s 4s/step - d1_loss: 0.2038 - d2_loss: 0.2782 - g_loss: 2.4067\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2204 - d2_loss: 0.3075 - g_loss: 2.4328\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2096 - d2_loss: 0.2883 - g_loss: 2.3239\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2225 - d2_loss: 0.3508 - g_loss: 2.7111\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2088 - d2_loss: 0.2799 - g_loss: 2.4373\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2090 - d2_loss: 0.2869 - g_loss: 2.0088\n",
      "4/4 [==============================] - 18s 5s/step - d1_loss: 0.2108 - d2_loss: 0.3349 - g_loss: 2.2516\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2131 - d2_loss: 0.3100 - g_loss: 2.2524\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2108 - d2_loss: 0.2894 - g_loss: 2.3806\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2061 - d2_loss: 0.2830 - g_loss: 2.3832\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2320 - d2_loss: 0.3095 - g_loss: 3.1247\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2098 - d2_loss: 0.2943 - g_loss: 2.3180\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2081 - d2_loss: 0.2840 - g_loss: 2.3817\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2072 - d2_loss: 0.2863 - g_loss: 2.3659\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2069 - d2_loss: 0.2681 - g_loss: 2.2556\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2161 - d2_loss: 0.3019 - g_loss: 2.5753\n",
      "4/4 [==============================] - 18s 5s/step - d1_loss: 0.2040 - d2_loss: 0.2709 - g_loss: 2.3695\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2183 - d2_loss: 0.2770 - g_loss: 2.7933\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2598 - d2_loss: 0.3904 - g_loss: 3.3167\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2135 - d2_loss: 0.3277 - g_loss: 2.3504\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2142 - d2_loss: 0.2849 - g_loss: 2.5163\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2080 - d2_loss: 0.2765 - g_loss: 2.4169\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2048 - d2_loss: 0.2782 - g_loss: 2.2645\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2055 - d2_loss: 0.2622 - g_loss: 2.5603\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2342 - d2_loss: 0.3641 - g_loss: 2.1674\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2099 - d2_loss: 0.3167 - g_loss: 2.3104\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2061 - d2_loss: 0.2793 - g_loss: 2.4796\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2072 - d2_loss: 0.2877 - g_loss: 2.3867\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2071 - d2_loss: 0.2904 - g_loss: 2.3901\n",
      "4/4 [==============================] - 18s 4s/step - d1_loss: 0.2040 - d2_loss: 0.2811 - g_loss: 2.3172\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 4,314,753\n",
      "Trainable params: 4,314,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_noise_input (Input [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8192)              1048576   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 256)         3276800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        51200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 3)         2403      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 5,437,667\n",
      "Trainable params: 5,420,323\n",
      "Non-trainable params: 17,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model = build_discriminator()\n",
    "g_model = build_generator(latent_dim)\n",
    "\n",
    "if os.path.exists('../models/saved_model/d_model.h5') and os.path.exists('../models/saved_model/g_model.h5'):\n",
    "    d_model.load_weights(\"../models/saved_model/d_model.h5\")\n",
    "    g_model.load_weights(\"../models/saved_model/g_model.h5\")\n",
    "else:\n",
    "    gan = GAN(d_model, g_model, latent_dim)\n",
    "    bce_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    gan.compile(d_optimizer, g_optimizer, bce_loss_fn)\n",
    "    images_dataset = tf_dataset(images_path, batch_size)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        gan.fit(images_dataset, epochs=1)\n",
    "        g_model.save(\"../models/saved_model/g_model.h5\")\n",
    "        d_model.save(\"../models/saved_model/d_model.h5\")\n",
    "\n",
    "        n_samples = 25\n",
    "        noise = np.random.normal(size=(n_samples, latent_dim))\n",
    "        examples = g_model.predict(noise)\n",
    "        save_plot(examples, epoch, int(np.sqrt(n_samples)))\n",
    "\n",
    "d_model.summary()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-lawrence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vertical-pillow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEeCAYAAABcyXrWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFF0lEQVR4nO3dwW7cVBiA0XhIaBdIsOEpukM8AeoD8OC8ABIrVFVIKKi0lDSZjM0SXbuYiCYfkXLO7soezyyiTzd/7Mm0LMsZQOnwf38A4OkRHiAnPEBOeICc8AA54QFy53sHv/v+5fC39rc//joc/315s3nN5evrYX397I9hfXozDevp4mZzjeP12MPls/Gc5bi6xmHeXGOex3PO1ufM43tM0/Yay7Lq8vqcZfUeZ25N4DG6y8/p6mf9MJ4zzc+H9ZdfbK9xuHgxrC9/+2H9xv/0bgAPT3iAnPAAuWnvkYlvX3wzHPzp9c/D8Zvb4+Y1H96/W73Dal4zn1bHP/JroMc44HG72O5Z1iPQ+fZkxgM8HsID5IQHyO3ex/Pq3athfVzNb67n7YxnMzNaTptzVi/YPw48PseP3Pe2n5OBHQ+QEx4gJzxATniA3O406P3l1bC+Wt0w6Puagb/d3vlMOx4gJzxATniA3O6M5zitbhA00gHugR0PkBMeICc8QG53xrPcjA+CLYY8wD2w4wFywgPkhAfI7c545mX7ZT8An8qOB8gJD5ATHiAnPEBuf7h8csMgcP/seICc8AA54QFy+w+JTuv/CvqQHwV4Kux4gJzwADnhAXK7Mx4zHeAh2PEAOeEBcsID5P5lxmPIA9w/Ox4gJzxATniAnPAAuf2HRKtPATwpdjxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5A73zs4TdOwXpblQT8M8DTY8QA54QFywgPkdmc8h8M44zmdzHiAT2fHA+SEB8gJD5DbnfE8O38+rK9Ofw5rEx/gv7DjAXLCA+SEB8gJD5DbHS5/9fXnw/rml3lYn+brzWuW2cgZnqaLO59pxwPkhAfICQ+Qm3y5F1Cz4wFywgPkhAfICQ+QEx4gJzxA7i+PQIlm5gpKsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_test_image(g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advanced-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_show_images(g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-reward",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
